{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ramil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ramil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('labeledTrainData.tsv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df.review.tolist()\n",
    "sentiment = df.sentiment.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount = 1000\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts[:amount], sentiment[:amount])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'stuff', 'go', 'moment', 'mj', 'start', 'listen', 'music', 'watch', 'odd', 'documentari', 'watch', 'the', 'wiz', 'watch', 'moonwalk', 'mayb', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'realli', 'cool', 'eighti', 'mayb', 'make', 'mind', 'whether', 'guilti', 'innoc', 'moonwalk', 'part', 'biographi', 'part', 'featur', 'film', 'rememb', 'go', 'see', 'cinema', 'origin', 'releas', 'some', 'subtl', 'messag', 'mj', 'feel', 'toward', 'press', 'also', 'obviou', 'messag', 'drug', 'bad', 'kay', 'visual', 'impress', 'cours', 'michael', 'jackson', 'unless', 'remot', 'like', 'mj', 'anyway', 'go', 'hate', 'find', 'bore', 'some', 'may', 'call', 'mj', 'egotist', 'consent', 'make', 'movi', 'but', 'mj', 'fan', 'would', 'say', 'made', 'fan', 'true', 'realli', 'nice', 'the', 'actual', 'featur', 'film', 'bit', 'final', 'start', '20', 'minut', 'exclud', 'smooth', 'crimin', 'sequenc', 'joe', 'pesci', 'convinc', 'psychopath', 'power', 'drug', 'lord', 'whi', 'want', 'mj', 'dead', 'bad', 'beyond', 'becaus', 'mj', 'overheard', 'plan', 'nah', 'joe', 'pesci', 'charact', 'rant', 'want', 'peopl', 'know', 'suppli', 'drug', 'etc', 'dunno', 'mayb', 'hate', 'mj', 'music', 'lot', 'cool', 'thing', 'like', 'mj', 'turn', 'car', 'robot', 'whole', 'speed', 'demon', 'sequenc', 'also', 'director', 'must', 'patienc', 'saint', 'came', 'film', 'kiddi', 'bad', 'sequenc', 'usual', 'director', 'hate', 'work', 'one', 'kid', 'let', 'alon', 'whole', 'bunch', 'perform', 'complex', 'danc', 'scene', 'bottom', 'line', 'movi', 'peopl', 'like', 'mj', 'one', 'level', 'anoth', 'think', 'peopl', 'if', 'stay', 'away', 'it', 'tri', 'give', 'wholesom', 'messag', 'iron', 'mj', 'bestest', 'buddi', 'movi', 'girl', 'michael', 'jackson', 'truli', 'one', 'talent', 'peopl', 'ever', 'grace', 'planet', 'guilti', 'well', 'attent', 'gave', 'subject', 'hmmm', 'well', 'know', 'peopl', 'differ', 'behind', 'close', 'door', 'know', 'fact', 'he', 'either', 'extrem', 'nice', 'stupid', 'guy', 'one', 'sickest', 'liar', 'i', 'hope', 'latter']\n"
     ]
    }
   ],
   "source": [
    "# clean text\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "\n",
    "def cleanhtml(text):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', text)\n",
    "    return cleantext\n",
    "\n",
    "def word_tokenize(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')#fix\n",
    "    clean = tokenizer.tokenize(text)\n",
    "    return clean\n",
    "   \n",
    "def stop_words(tokenize_words):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    wordsFiltered = []\n",
    "    for w in tokenize_words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w.lower())\n",
    "    return wordsFiltered\n",
    "\n",
    "\n",
    "def stem(text_arr):\n",
    "    ps = PorterStemmer()\n",
    "    stem_arr = []\n",
    "    for word in text_arr:\n",
    "        stem_arr.append(ps.stem(word))\n",
    "    return stem_arr\n",
    "\n",
    "def preprocess_text(text):\n",
    "    #clean html \n",
    "    clean = cleanhtml(text)\n",
    "    \n",
    "    # tokenize and remove punctuation \n",
    "    tokenize = word_tokenize(clean)\n",
    "    \n",
    "    # stop words\n",
    "    stop = stop_words(tokenize)\n",
    "    \n",
    "    # stem \n",
    "    stem_arr = stem(stop)\n",
    "    \n",
    "    return stem_arr\n",
    "\n",
    "\n",
    "def preprocess_texts(texts):\n",
    "    tmp_set = set()\n",
    "    for text in texts:\n",
    "        arr = preprocess_text(text) \n",
    "        tmp_set.update(arr)\n",
    "    return tmp_set\n",
    "\n",
    "print(preprocess_text(df.to_dict('records')[0]['review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# print(train_texts)\n",
    "corpus = X_train\n",
    "\n",
    "vocab_tf = preprocess_texts(corpus)\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words = 'english', vocabulary=vocab_tf)\n",
    "X = tfidf.fit_transform(corpus)\n",
    "X_dense = X.todense()\n",
    "\n",
    "# print(tfidf.get_feature_names())\n",
    "X_test_transform = tfidf.transform(X_test)\n",
    "X_test_transform_dense = X_test_transform.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n clusters = 2, silhouette score is 0.002737106606338904)\n",
      "n clusters = 3, silhouette score is 0.001292570041600025)\n",
      "n clusters = 4, silhouette score is 0.0012257763295509033)\n",
      "n clusters = 5, silhouette score is 0.0019496912820422922)\n",
      "n clusters = 6, silhouette score is -0.00043158609389703454)\n",
      "n clusters = 7, silhouette score is 0.0003233193644542233)\n",
      "n clusters = 8, silhouette score is -0.0003621400046931758)\n",
      "n clusters = 9, silhouette score is 0.001089552518658691)\n"
     ]
    }
   ],
   "source": [
    "# KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "clusters = range(2,10)\n",
    "\n",
    "for n_clusters in clusters:\n",
    "    clusterer = KMeans(n_clusters=n_clusters)\n",
    "    preds = clusterer.fit_predict(X_dense)\n",
    "    centers = clusterer.cluster_centers_\n",
    "\n",
    "    score = silhouette_score(X_dense, preds)\n",
    "    print(\"n clusters = {}, silhouette score is {})\".format(n_clusters, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n clusters = 2, silhouette score is 0.011538330989623735)\n",
      "n clusters = 3, silhouette score is 0.00715896005739769)\n",
      "n clusters = 4, silhouette score is 0.005223290192891512)\n",
      "n clusters = 5, silhouette score is 0.0038835965138133825)\n",
      "n clusters = 6, silhouette score is 0.003278005709045163)\n",
      "n clusters = 7, silhouette score is 0.002390840197083303)\n",
      "n clusters = 8, silhouette score is 0.0016568445207679472)\n",
      "n clusters = 9, silhouette score is 0.0016462717331154875)\n",
      "n clusters = 10, silhouette score is 0.0011130159379573355)\n",
      "n clusters = 11, silhouette score is 0.0011784284873401698)\n",
      "n clusters = 12, silhouette score is 0.000567130971221925)\n",
      "n clusters = 13, silhouette score is 0.0008488792337937323)\n",
      "n clusters = 14, silhouette score is 0.0007325382721427397)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering    \n",
    "\n",
    "clusters = range(2,15)\n",
    "\n",
    "for n_clusters in clusters:\n",
    "    clusterer = AgglomerativeClustering(n_clusters=n_clusters, linkage='average')\n",
    "    preds = clusterer.fit_predict(X_dense)\n",
    "\n",
    "    score = silhouette_score(X_dense, preds)\n",
    "    print(\"n clusters = {}, silhouette score is {})\".format(n_clusters, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  1.00429122e-03  2.14117761e-03 ...  3.79668117e-04\n",
      "   4.06999625e-04  5.41529511e-04]\n",
      " [ 0.00000000e+00  4.52968325e-04  8.60821474e-04 ...  2.98155597e-19\n",
      "   4.33680869e-19 -1.62630326e-19]]\n",
      "11404\n"
     ]
    }
   ],
   "source": [
    "max_clusterer = KMeans(n_clusters=2)\n",
    "max_preds = max_clusterer.fit_predict(X_dense)\n",
    "max_centers = max_clusterer.cluster_centers_\n",
    "print(max_centers)\n",
    "print(len(max_centers[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_tf contains vocab\n",
    "# max_preds pred\n",
    "# corpus - texts \n",
    "clean_texts = []\n",
    "for text_idx in range(len(corpus)):\n",
    "    clean_texts.append({'words': preprocess_text(corpus[text_idx]), 'ans': max_preds[text_idx]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540 210 750\n"
     ]
    }
   ],
   "source": [
    "pos_amount = sum(max_preds)\n",
    "neg_amount = len(max_preds) - pos_amount\n",
    "print(pos_amount, neg_amount, len(max_preds))\n",
    "vocab_tf_list = list(vocab_tf)\n",
    "vocab_tf_list_range = range(len(vocab_tf_list))\n",
    "pos_amount_words = [0 for i in vocab_tf_list_range]\n",
    "neg_amount_words = [0 for i in vocab_tf_list_range]\n",
    "all_amount_words = [0 for i in vocab_tf_list_range]\n",
    "for vocab_word_idx in vocab_tf_list_range:\n",
    "    for text in clean_texts:\n",
    "        if(vocab_tf_list[vocab_word_idx] in text['words']):\n",
    "            all_amount_words[vocab_word_idx] += 1\n",
    "            if text['ans'] == 1:\n",
    "                pos_amount_words[vocab_word_idx] += 1\n",
    "            else: \n",
    "                neg_amount_words[vocab_word_idx] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def pmiCalc(classProb, allProb):\n",
    "    if classProb == 0 or allProb == 0: \n",
    "        return 0\n",
    "    return math.log10(classProb / allProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pmi = []\n",
    "pos_pmi = []\n",
    "for idx in vocab_tf_list_range:\n",
    "    pos_pmi_calc = pmiCalc(pos_amount_words[idx], all_amount_words[idx])\n",
    "    if pos_pmi_calc != 0:\n",
    "        pos_pmi.append({'idx': idx, 'pmi': pos_pmi_calc})  \n",
    "    \n",
    "    neg_pmi_calc = pmiCalc(neg_amount_words[idx], all_amount_words[idx])\n",
    "    if neg_pmi_calc != 0:\n",
    "        neg_pmi.append({'idx': idx, 'pmi': neg_pmi_calc})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 positive words\n",
      "adventur\n",
      "train\n",
      "everybodi\n",
      "u\n",
      "mouth\n",
      "dad\n",
      "radio\n",
      "interpret\n",
      "70\n",
      "5\n",
      "------------\n",
      "Top 10 nagetive words\n",
      "context\n",
      "elev\n",
      "fascism\n",
      "importantli\n",
      "festiv\n",
      "devoid\n",
      "merit\n",
      "newcom\n",
      "spanish\n",
      "loneli\n"
     ]
    }
   ],
   "source": [
    "sorted_pos_pmi = sorted(pos_pmi, key = lambda x : x['pmi'], reverse = True)\n",
    "sorted_neg_pmi = sorted(neg_pmi, key = lambda x : x['pmi'], reverse = True)\n",
    "\n",
    "top_amount = 10\n",
    "print(f'Top {top_amount} positive words')\n",
    "for word in range(top_amount):\n",
    "    print(vocab_tf_list[sorted_pos_pmi[word]['idx']])\n",
    "print('------------')\n",
    "print(f'Top {top_amount} nagetive words')\n",
    "for word in range(top_amount):\n",
    "    print(vocab_tf_list[sorted_neg_pmi[word]['idx']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
