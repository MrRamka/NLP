{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ramil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ramil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('labeledTrainData.tsv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df.review.tolist()\n",
    "sentiment = df.sentiment.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount = 1000\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts[:amount], sentiment[:amount])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'stuff', 'go', 'moment', 'mj', 'start', 'listen', 'music', 'watch', 'odd', 'documentari', 'watch', 'the', 'wiz', 'watch', 'moonwalk', 'mayb', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'realli', 'cool', 'eighti', 'mayb', 'make', 'mind', 'whether', 'guilti', 'innoc', 'moonwalk', 'part', 'biographi', 'part', 'featur', 'film', 'rememb', 'go', 'see', 'cinema', 'origin', 'releas', 'some', 'subtl', 'messag', 'mj', 'feel', 'toward', 'press', 'also', 'obviou', 'messag', 'drug', 'bad', 'kay', 'visual', 'impress', 'cours', 'michael', 'jackson', 'unless', 'remot', 'like', 'mj', 'anyway', 'go', 'hate', 'find', 'bore', 'some', 'may', 'call', 'mj', 'egotist', 'consent', 'make', 'movi', 'but', 'mj', 'fan', 'would', 'say', 'made', 'fan', 'true', 'realli', 'nice', 'the', 'actual', 'featur', 'film', 'bit', 'final', 'start', '20', 'minut', 'exclud', 'smooth', 'crimin', 'sequenc', 'joe', 'pesci', 'convinc', 'psychopath', 'power', 'drug', 'lord', 'whi', 'want', 'mj', 'dead', 'bad', 'beyond', 'becaus', 'mj', 'overheard', 'plan', 'nah', 'joe', 'pesci', 'charact', 'rant', 'want', 'peopl', 'know', 'suppli', 'drug', 'etc', 'dunno', 'mayb', 'hate', 'mj', 'music', 'lot', 'cool', 'thing', 'like', 'mj', 'turn', 'car', 'robot', 'whole', 'speed', 'demon', 'sequenc', 'also', 'director', 'must', 'patienc', 'saint', 'came', 'film', 'kiddi', 'bad', 'sequenc', 'usual', 'director', 'hate', 'work', 'one', 'kid', 'let', 'alon', 'whole', 'bunch', 'perform', 'complex', 'danc', 'scene', 'bottom', 'line', 'movi', 'peopl', 'like', 'mj', 'one', 'level', 'anoth', 'think', 'peopl', 'if', 'stay', 'away', 'it', 'tri', 'give', 'wholesom', 'messag', 'iron', 'mj', 'bestest', 'buddi', 'movi', 'girl', 'michael', 'jackson', 'truli', 'one', 'talent', 'peopl', 'ever', 'grace', 'planet', 'guilti', 'well', 'attent', 'gave', 'subject', 'hmmm', 'well', 'know', 'peopl', 'differ', 'behind', 'close', 'door', 'know', 'fact', 'he', 'either', 'extrem', 'nice', 'stupid', 'guy', 'one', 'sickest', 'liar', 'i', 'hope', 'latter']\n"
     ]
    }
   ],
   "source": [
    "# clean text\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "\n",
    "def cleanhtml(text):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', text)\n",
    "    return cleantext\n",
    "\n",
    "def word_tokenize(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')#fix\n",
    "    clean = tokenizer.tokenize(text)\n",
    "    return clean\n",
    "   \n",
    "def stop_words(tokenize_words):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    wordsFiltered = []\n",
    "    for w in tokenize_words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w.lower())\n",
    "    return wordsFiltered\n",
    "\n",
    "\n",
    "def stem(text_arr):\n",
    "    ps = PorterStemmer()\n",
    "    stem_arr = []\n",
    "    for word in text_arr:\n",
    "        stem_arr.append(ps.stem(word))\n",
    "    return stem_arr\n",
    "\n",
    "def preprocess_text(text):\n",
    "    #clean html \n",
    "    clean = cleanhtml(text)\n",
    "    \n",
    "    # tokenize and remove punctuation \n",
    "    tokenize = word_tokenize(clean)\n",
    "    \n",
    "    # stop words\n",
    "    stop = stop_words(tokenize)\n",
    "    \n",
    "    # stem \n",
    "    stem_arr = stem(stop)\n",
    "    \n",
    "    return stem_arr\n",
    "\n",
    "\n",
    "def preprocess_texts(texts):\n",
    "    tmp_set = set()\n",
    "    for text in texts:\n",
    "        arr = preprocess_text(text) \n",
    "        tmp_set.update(arr)\n",
    "    return tmp_set\n",
    "\n",
    "print(preprocess_text(df.to_dict('records')[0]['review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# print(train_texts)\n",
    "corpus = X_train\n",
    "\n",
    "vocab_tf = preprocess_texts(corpus)\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words = 'english', vocabulary=vocab_tf)\n",
    "X = tfidf.fit_transform(corpus)\n",
    "X_dense = X.todense()\n",
    "\n",
    "# print(tfidf.get_feature_names())\n",
    "X_test_transform = tfidf.transform(X_test)\n",
    "X_test_transform_dense = X_test_transform.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.83      0.76       120\n",
      "           1       0.81      0.66      0.73       130\n",
      "\n",
      "    accuracy                           0.74       250\n",
      "   macro avg       0.75      0.75      0.74       250\n",
      "weighted avg       0.76      0.74      0.74       250\n",
      "\n",
      "n clusters = 100, aaccuracy = 0.568\n",
      "n clusters = 110, aaccuracy = 0.54\n",
      "n clusters = 120, aaccuracy = 0.512\n",
      "n clusters = 130, aaccuracy = 0.512\n",
      "n clusters = 140, aaccuracy = 0.54\n",
      "n clusters = 150, aaccuracy = 0.528\n",
      "n clusters = 160, aaccuracy = 0.544\n",
      "n clusters = 170, aaccuracy = 0.524\n",
      "n clusters = 180, aaccuracy = 0.572\n",
      "n clusters = 190, aaccuracy = 0.568\n",
      "n clusters = 200, aaccuracy = 0.56\n",
      "n clusters = 210, aaccuracy = 0.536\n",
      "n clusters = 220, aaccuracy = 0.504\n",
      "n clusters = 230, aaccuracy = 0.516\n",
      "n clusters = 240, aaccuracy = 0.524\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "n = range(100, 250, 10)\n",
    "\n",
    "temp = []\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X_dense, y_train)\n",
    "pred = clf.predict(X_test_transform_dense)\n",
    "print(classification_report(pred, y_test[:amount]))\n",
    "\n",
    "for n_components in n:\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X_dense)\n",
    "    ans = pca.transform(X_dense)\n",
    "    \n",
    "    pca.fit(X_test_transform_dense)\n",
    "    ans_test = pca.transform(X_test_transform_dense)\n",
    "    \n",
    "    \n",
    "    clf = LogisticRegression(random_state=0).fit(ans, y_train)\n",
    "    pred = clf.predict(ans_test)\n",
    "    report = classification_report(pred, y_test[:amount], output_dict=True)\n",
    "\n",
    "    print(f'n clusters = {n_components}, aaccuracy = {report[\"accuracy\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from bs4 import BeautifulSoup\n",
    "import re, string\n",
    "clean = [ ]\n",
    "\n",
    "for doc in texts:\n",
    "    x = doc.lower()                     \n",
    "    x = BeautifulSoup(x, 'lxml').text   \n",
    "    x = re.sub('[^A-Za-z0-9]+', ' ', x)\n",
    "    x = x.split(' ')\n",
    "    clean.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again', 'maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent', 'moonwalker', 'is', 'part', 'biography', 'part', 'feature', 'film', 'which', 'i', 'remember', 'going', 'to', 'see', 'at', 'the', 'cinema', 'when', 'it', 'was', 'originally', 'released', 'some', 'of', 'it', 'has', 'subtle', 'messages', 'about', 'mj', 's', 'feeling', 'towards', 'the', 'press', 'and', 'also', 'the', 'obvious', 'message', 'of', 'drugs', 'are', 'bad', 'm', 'kay', 'visually', 'impressive', 'but', 'of', 'course', 'this', 'is', 'all', 'about', 'michael', 'jackson', 'so', 'unless', 'you', 'remotely', 'like', 'mj', 'in', 'anyway', 'then', 'you', 'are', 'going', 'to', 'hate', 'this', 'and', 'find', 'it', 'boring', 'some', 'may', 'call', 'mj', 'an', 'egotist', 'for', 'consenting', 'to', 'the', 'making', 'of', 'this', 'movie', 'but', 'mj', 'and', 'most', 'of', 'his', 'fans', 'would', 'say', 'that', 'he', 'made', 'it', 'for', 'the', 'fans', 'which', 'if', 'true', 'is', 'really', 'nice', 'of', 'him', 'the', 'actual', 'feature', 'film', 'bit', 'when', 'it', 'finally', 'starts', 'is', 'only', 'on', 'for', '20', 'minutes', 'or', 'so', 'excluding', 'the', 'smooth', 'criminal', 'sequence', 'and', 'joe', 'pesci', 'is', 'convincing', 'as', 'a', 'psychopathic', 'all', 'powerful', 'drug', 'lord', 'why', 'he', 'wants', 'mj', 'dead', 'so', 'bad', 'is', 'beyond', 'me', 'because', 'mj', 'overheard', 'his', 'plans', 'nah', 'joe', 'pesci', 's', 'character', 'ranted', 'that', 'he', 'wanted', 'people', 'to', 'know', 'it', 'is', 'he', 'who', 'is', 'supplying', 'drugs', 'etc', 'so', 'i', 'dunno', 'maybe', 'he', 'just', 'hates', 'mj', 's', 'music', 'lots', 'of', 'cool', 'things', 'in', 'this', 'like', 'mj', 'turning', 'into', 'a', 'car', 'and', 'a', 'robot', 'and', 'the', 'whole', 'speed', 'demon', 'sequence', 'also', 'the', 'director', 'must', 'have', 'had', 'the', 'patience', 'of', 'a', 'saint', 'when', 'it', 'came', 'to', 'filming', 'the', 'kiddy', 'bad', 'sequence', 'as', 'usually', 'directors', 'hate', 'working', 'with', 'one', 'kid', 'let', 'alone', 'a', 'whole', 'bunch', 'of', 'them', 'performing', 'a', 'complex', 'dance', 'scene', 'bottom', 'line', 'this', 'movie', 'is', 'for', 'people', 'who', 'like', 'mj', 'on', 'one', 'level', 'or', 'another', 'which', 'i', 'think', 'is', 'most', 'people', 'if', 'not', 'then', 'stay', 'away', 'it', 'does', 'try', 'and', 'give', 'off', 'a', 'wholesome', 'message', 'and', 'ironically', 'mj', 's', 'bestest', 'buddy', 'in', 'this', 'movie', 'is', 'a', 'girl', 'michael', 'jackson', 'is', 'truly', 'one', 'of', 'the', 'most', 'talented', 'people', 'ever', 'to', 'grace', 'this', 'planet', 'but', 'is', 'he', 'guilty', 'well', 'with', 'all', 'the', 'attention', 'i', 've', 'gave', 'this', 'subject', 'hmmm', 'well', 'i', 'don', 't', 'know', 'because', 'people', 'can', 'be', 'different', 'behind', 'closed', 'doors', 'i', 'know', 'this', 'for', 'a', 'fact', 'he', 'is', 'either', 'an', 'extremely', 'nice', 'but', 'stupid', 'guy', 'or', 'one', 'of', 'the', 'most', 'sickest', 'liars', 'i', 'hope', 'he', 'is', 'not', 'the', 'latter', '']\n"
     ]
    }
   ],
   "source": [
    "print(clean[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences=clean, vector_size=200, window=10, min_count=10, negative=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "market [('studios', 0.6523382663726807), ('usa', 0.6327494978904724), ('access', 0.5971789360046387), ('columbia', 0.59209144115448), ('blockbuster', 0.5912190079689026), ('company', 0.5869526267051697), ('stores', 0.5834096670150757), ('cinemas', 0.5787006616592407), ('distributor', 0.5743589997291565), ('national', 0.573961079120636)]\n",
      "\n",
      "street [('pickup', 0.7195087671279907), ('park', 0.7136522531509399), ('elm', 0.7083839178085327), ('streets', 0.6889354586601257), ('riding', 0.6753363609313965), ('beach', 0.6732529997825623), ('horseback', 0.6650832295417786), ('roof', 0.6633862853050232), ('mountain', 0.6598338484764099), ('nightmare', 0.6591573357582092)]\n",
      "\n",
      "game [('games', 0.6758685111999512), ('baseball', 0.5117459297180176), ('football', 0.4533810615539551), ('basketball', 0.4491329491138458), ('mouse', 0.444535493850708), ('64', 0.4358806312084198), ('missions', 0.41914787888526917), ('store', 0.40829816460609436), ('nintendo', 0.4081835150718689), ('clip', 0.40179628133773804)]\n",
      "\n",
      "lesson [('religion', 0.5361049771308899), ('statement', 0.5357525944709778), ('lessons', 0.4925806522369385), ('message', 0.4855845272541046), ('society', 0.4816719889640808), ('teach', 0.46748635172843933), ('learning', 0.4662938714027405), ('learn', 0.4606195390224457), ('puerto', 0.4575253427028656), ('humanity', 0.45319679379463196)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "market = model.wv.most_similar('market')\n",
    "print(\"market\", market)\n",
    "print()\n",
    "\n",
    "street = model.wv.most_similar('street')\n",
    "print(\"street\", street)\n",
    "print()\n",
    "\n",
    "game = model.wv.most_similar('game')\n",
    "print(\"game\", game)\n",
    "print()\n",
    "\n",
    "lesson = model.wv.most_similar('lesson')\n",
    "print(\"lesson\", lesson)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.11379233]]\n",
      "[[0.         1.05969149 1.07309676]\n",
      " [0.         1.31334844 1.31999004]\n",
      " [0.         1.2416603  1.33262491]\n",
      " [0.         1.25635501 1.28977382]\n",
      " [0.         1.30840454 1.31359879]\n",
      " [0.         1.28927059 1.29435016]\n",
      " [0.         1.33713325 1.35164183]\n",
      " [0.         1.31259271 1.31421363]\n",
      " [0.         1.30682287 1.31005683]\n",
      " [0.         1.07730653 1.09457385]]\n",
      "[[  0 481 474]\n",
      " [  1 253 303]\n",
      " [  2  89 128]\n",
      " [  3 676 637]\n",
      " [  4 531 331]\n",
      " [  5 645 390]\n",
      " [  6  46 566]\n",
      " [  7 503 670]\n",
      " [  8 303 108]\n",
      " [  9 297 485]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "print(X_dense[:10])\n",
    "tree = KDTree(X_dense)\n",
    "dist, ind = tree.query(X_dense[:10], k=3) \n",
    "print(dist)\n",
    "print(ind)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(corpus):\n",
    "    # initialize\n",
    "    clean_text = []\n",
    "\n",
    "    for row in corpus:\n",
    "        # tokenize\n",
    "        tokens = nltk.tokenize.word_tokenize(row)\n",
    "        # lowercase\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        # isword\n",
    "        tokens = [token for token in tokens if token.isalpha()]\n",
    "        clean_sentence = ''\n",
    "        clean_sentence = ' '.join(token for token in tokens)\n",
    "        clean_text.append(clean_sentence)\n",
    "        \n",
    "    return clean_text\n",
    "    \n",
    "all_text = preprocessing(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Convert a collection of text documents to a matrix of token counts\n",
    "cv = CountVectorizer(ngram_range=(1,1), stop_words = 'english')\n",
    "# matrix of token counts\n",
    "X = cv.fit_transform(all_text)\n",
    "Xc = (X.T * X) # matrix manipulation\n",
    "Xc.setdiag(0) # set the diagonals to be zeroes as it's pointless to be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37953\n"
     ]
    }
   ],
   "source": [
    "names = cv.get_feature_names()\n",
    "print(names.index('market'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "clf = TruncatedSVD(100)\n",
    "Xpca = clf.fit_transform(Xc)\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(Xpca)\n",
    "ans = pca.transform(Xpca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(ans[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = KDTree(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[37953 46630 67941 48588 51353]]\n",
      "market\n",
      "placed\n",
      "welcome\n",
      "provided\n",
      "reputation\n"
     ]
    }
   ],
   "source": [
    "market = names.index('market')\n",
    "dist, ind = tree.query([ans[market]], k=5) \n",
    "print(ind)\n",
    "for i in ind:\n",
    "    for j in i:\n",
    "        print(names[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[59429  8267 18064 51947 23115]]\n",
      "street\n",
      "business\n",
      "dream\n",
      "rich\n",
      "following\n"
     ]
    }
   ],
   "source": [
    "street = names.index('street')\n",
    "dist, ind = tree.query([ans[street]], k=5) \n",
    "print(ind)\n",
    "for i in ind:\n",
    "    for j in i:\n",
    "        print(names[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24365 22279 27170 52991 68735]]\n",
      "game\n",
      "fight\n",
      "hand\n",
      "run\n",
      "wo\n"
     ]
    }
   ],
   "source": [
    "game = names.index('game')\n",
    "dist, ind = tree.query([ans[game]], k=5) \n",
    "print(ind)\n",
    "for i in ind:\n",
    "    for j in i:\n",
    "        print(names[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35526 29019 45126 60507 50390]]\n",
      "lesson\n",
      "hopes\n",
      "passed\n",
      "survive\n",
      "record\n"
     ]
    }
   ],
   "source": [
    "lesson = names.index('lesson')\n",
    "dist, ind = tree.query([ans[lesson]], k=5) \n",
    "print(ind)\n",
    "for i in ind:\n",
    "    for j in i:\n",
    "        print(names[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
