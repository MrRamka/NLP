{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ramil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ramil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('labeledTrainData.tsv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df.review.tolist()\n",
    "sentiment = df.sentiment.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount = 1000\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts[:amount], sentiment[:amount])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'stuff', 'go', 'moment', 'mj', 'start', 'listen', 'music', 'watch', 'odd', 'documentari', 'watch', 'the', 'wiz', 'watch', 'moonwalk', 'mayb', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'realli', 'cool', 'eighti', 'mayb', 'make', 'mind', 'whether', 'guilti', 'innoc', 'moonwalk', 'part', 'biographi', 'part', 'featur', 'film', 'rememb', 'go', 'see', 'cinema', 'origin', 'releas', 'some', 'subtl', 'messag', 'mj', 'feel', 'toward', 'press', 'also', 'obviou', 'messag', 'drug', 'bad', 'kay', 'visual', 'impress', 'cours', 'michael', 'jackson', 'unless', 'remot', 'like', 'mj', 'anyway', 'go', 'hate', 'find', 'bore', 'some', 'may', 'call', 'mj', 'egotist', 'consent', 'make', 'movi', 'but', 'mj', 'fan', 'would', 'say', 'made', 'fan', 'true', 'realli', 'nice', 'the', 'actual', 'featur', 'film', 'bit', 'final', 'start', '20', 'minut', 'exclud', 'smooth', 'crimin', 'sequenc', 'joe', 'pesci', 'convinc', 'psychopath', 'power', 'drug', 'lord', 'whi', 'want', 'mj', 'dead', 'bad', 'beyond', 'becaus', 'mj', 'overheard', 'plan', 'nah', 'joe', 'pesci', 'charact', 'rant', 'want', 'peopl', 'know', 'suppli', 'drug', 'etc', 'dunno', 'mayb', 'hate', 'mj', 'music', 'lot', 'cool', 'thing', 'like', 'mj', 'turn', 'car', 'robot', 'whole', 'speed', 'demon', 'sequenc', 'also', 'director', 'must', 'patienc', 'saint', 'came', 'film', 'kiddi', 'bad', 'sequenc', 'usual', 'director', 'hate', 'work', 'one', 'kid', 'let', 'alon', 'whole', 'bunch', 'perform', 'complex', 'danc', 'scene', 'bottom', 'line', 'movi', 'peopl', 'like', 'mj', 'one', 'level', 'anoth', 'think', 'peopl', 'if', 'stay', 'away', 'it', 'tri', 'give', 'wholesom', 'messag', 'iron', 'mj', 'bestest', 'buddi', 'movi', 'girl', 'michael', 'jackson', 'truli', 'one', 'talent', 'peopl', 'ever', 'grace', 'planet', 'guilti', 'well', 'attent', 'gave', 'subject', 'hmmm', 'well', 'know', 'peopl', 'differ', 'behind', 'close', 'door', 'know', 'fact', 'he', 'either', 'extrem', 'nice', 'stupid', 'guy', 'one', 'sickest', 'liar', 'i', 'hope', 'latter']\n"
     ]
    }
   ],
   "source": [
    "# clean text\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "\n",
    "def cleanhtml(text):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', text)\n",
    "    return cleantext\n",
    "\n",
    "def word_tokenize(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')#fix\n",
    "    clean = tokenizer.tokenize(text)\n",
    "    return clean\n",
    "   \n",
    "def stop_words(tokenize_words):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    wordsFiltered = []\n",
    "    for w in tokenize_words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w.lower())\n",
    "    return wordsFiltered\n",
    "\n",
    "\n",
    "def stem(text_arr):\n",
    "    ps = PorterStemmer()\n",
    "    stem_arr = []\n",
    "    for word in text_arr:\n",
    "        stem_arr.append(ps.stem(word))\n",
    "    return stem_arr\n",
    "\n",
    "def preprocess_text(text):\n",
    "    #clean html \n",
    "    clean = cleanhtml(text)\n",
    "    \n",
    "    # tokenize and remove punctuation \n",
    "    tokenize = word_tokenize(clean)\n",
    "    \n",
    "    # stop words\n",
    "    stop = stop_words(tokenize)\n",
    "    \n",
    "    # stem \n",
    "    stem_arr = stem(stop)\n",
    "    \n",
    "    return stem_arr\n",
    "\n",
    "\n",
    "def preprocess_texts(texts):\n",
    "    tmp_set = set()\n",
    "    for text in texts:\n",
    "        arr = preprocess_text(text) \n",
    "        tmp_set.update(arr)\n",
    "    return tmp_set\n",
    "\n",
    "print(preprocess_text(df.to_dict('records')[0]['review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# print(train_texts)\n",
    "corpus = X_train\n",
    "\n",
    "vocab_tf = preprocess_texts(corpus)\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words = 'english', vocabulary=vocab_tf)\n",
    "X = tfidf.fit_transform(corpus)\n",
    "X_dense = X.todense()\n",
    "\n",
    "# print(tfidf.get_feature_names())\n",
    "X_test_transform = tfidf.transform(X_test)\n",
    "X_test_transform_dense = X_test_transform.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.76      0.79       136\n",
      "           1       0.74      0.79      0.76       114\n",
      "\n",
      "    accuracy                           0.78       250\n",
      "   macro avg       0.78      0.78      0.78       250\n",
      "weighted avg       0.78      0.78      0.78       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X_dense, y_train)\n",
    "pred = clf.predict(X_test_transform_dense)\n",
    "print(classification_report(pred, y_test[:amount]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.67      0.65       123\n",
      "           1       0.66      0.64      0.65       127\n",
      "\n",
      "    accuracy                           0.65       250\n",
      "   macro avg       0.65      0.65      0.65       250\n",
      "weighted avg       0.65      0.65      0.65       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dclf = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "dclf.fit(X_dense, y_train)      # Use fit method on the train data\n",
    "\n",
    "dtc_pred = dclf.predict(X_test_transform_dense)   # Predict the target class of test data\n",
    "print(classification_report(dtc_pred, y_test[:amount])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.61      0.64       140\n",
      "           1       0.56      0.62      0.59       110\n",
      "\n",
      "    accuracy                           0.62       250\n",
      "   macro avg       0.61      0.62      0.61       250\n",
      "weighted avg       0.62      0.62      0.62       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X_dense, y_train)\n",
    "neigh_pred = neigh.predict(X_test_transform_dense)\n",
    "print(classification_report(neigh_pred, y_test[:amount]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
